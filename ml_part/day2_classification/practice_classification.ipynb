{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Binary nearest centroid classifier\n",
    "\n",
    "#### Implement the Nearest centroid classifier by following these steps:\n",
    "####  1. Implement a euclidena distance function, given by the following formula:\n",
    "$dist(\\mathbf x, \\mathbf y)=\\lvert\\lvert \\mathbf x - \\mathbf y \\lvert\\lvert^{2}$\n",
    "####  2. Write a class, that represents a Binary nearest centroid classifier, which should contain:\n",
    "- the init function should take as inputs a distance function(here we are gonna use your written function from task 1)\n",
    "- a fit function, that takes as input arguments a dataset and the correspodning labels and does the following:\n",
    "    - save the dataset and the labels separately as class variables(self. etc.)\n",
    "    - compute the means of both classes and save them as class variables\n",
    "    - extract the indices of points from each class and save those 2 lists as class variables\n",
    "- a predict function that takes as input a new datapoint and computes and return the corresponding label, specifically it should do the following:\n",
    "    - compute the shortest distance to one of the means, which will be the label that we assign to it. \n",
    "    - concatenate the old dataset with the new data point and its predict label, respectively\n",
    "    - recompute the mean class variables\n",
    "- Use the model to classify the newly provided data point\n",
    "- make a scatter plot of the new dataset, including the newly classified point. Use the label information to paint the points according to their color\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Plot the decision boundary for given dataset(Note: decision boundary is where the distance to both centroids is equally large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic dataset\n",
    "dog_data = pd.DataFrame(data={'good girl/boy': [2, 2.4, 2.5, 3], 'grumpiness': [.9, .9, 1.2 ,.8]})\n",
    "cat_data = pd.DataFrame(data={'good girl/boy': [0.10, 0.5, 0.7, 0.2], 'grumpiness': [2, 1.5 ,1.8, 1.7]})\n",
    "labels = ['dog', 'cat']\n",
    "\n",
    "X = np.vstack([dog_data, cat_data])\n",
    "Y = np.concatenate([np.zeros(len(dog_data)), np.ones(len(cat_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dog_data['good girl/boy'], dog_data['grumpiness'], label='Dogs')\n",
    "plt.scatter(cat_data['good girl/boy'], cat_data['grumpiness'], label='Cats')\n",
    "plt.xlabel('Grumpiness')\n",
    "plt.ylabel('Good Girl/Boy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_NC_classifier(object):\n",
    "    \n",
    "    def __init__(self, dist_func):\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_point = np.array([1.8, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112490369793977\n",
      "1.8955540087267364\n",
      "Predicted label: dog\n"
     ]
    }
   ],
   "source": [
    "predicted_label = model.predict(new_point)\n",
    "print(f'Predicted label: {labels[predicted_label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Binary breast cancer classification\n",
    "Following up you'll find a UCI dataset containing samples of healthy patients, and patients that were diagnosed with breast cancer. \n",
    "For specific information about the dataset, please visit: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "\n",
    "Your goals are to complete the following tasks:\n",
    "\n",
    "### 1. Parse the data and perform some cleaning:\n",
    "- Split the labels(='diagnosis') from the rest of the dataset\n",
    "- Drop the columns: 'id', 'Unnamed: 32':\n",
    "    - Hint: Check out the pandas function 'drop': https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "- Convert the labels(=['M', 'B']) to binary ones(=0,1): google for the pandas function:\n",
    "    - Hint: Use pd.get_dummies()\n",
    "- split the dataset into training and test set(ratio=80%/20%). Create two new datasets, and keep the original, whole dataset fixed:\n",
    "    - You may use: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    - Don't forget to shuffle the data\n",
    "\n",
    "### 2. Do some exploration on the whole dataset(training and test set together):\n",
    "- Plot histogram of radius mean values\n",
    "    - Hint: Use np.hist() \n",
    "- Plot correlation matrix between between every pair of features\n",
    "    - Hint: Use sns.heatmap(data.corr())\n",
    "- Look for feature pairs that strongly correlate with each other, and save one of them. Repeat this process until you have found 5 features. Keep those and drop the rest.(remember to do this for both the training dataset and the test dataset) \n",
    "    \n",
    "### 3. Use the sklearn package and train the following algorithms on the dataset:\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "- Apply the following linear models to the dataset:\n",
    "    - Logisitc regression\n",
    "    - SVM(linear kernel)\n",
    "    \n",
    "- Apply the following non-linear models to the dataset:\n",
    "    - SVM(rbf kernel)\n",
    "    - Random Forest\n",
    "        \n",
    "- NOTE: Test different hyperparameter settings for the corresponding models and pick the best one.\n",
    "    \n",
    "### 4. Compute the confusion matrix and print a classification report for each classifier, which one performs best and why would you say so ?\n",
    "- Docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html, \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/breast_cancer.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement cross validation\n",
    "![Binary classification](resources/crossvalidation.png)\n",
    "### TODO:\n",
    "Your first task will be to implement the cross-validation algorithm using a classification algorithm of your choice.\n",
    "The pseudocode would look something like this:\n",
    "\n",
    "```python\n",
    "1. split dataset into training set ant test set\n",
    "2. loop over a list of hyperparameter settings:\n",
    "    2.1 split training set into training set and validation set\n",
    "    2.2 initialize your classification algo. using those hyperparams\n",
    "    2.3 train your algo. on the training set\n",
    "    2.4 measure accuracy of your algo on the validation set\n",
    "    2.5 safe temp. hyperparams if temp accuracy exceeded prior one\n",
    "3. train the whole training set on the best hyperparameter setting\n",
    "4. evaluate final accuracy of your algorithm using the test set\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Multiclass sentiment Analysis a la Trump\n",
    "![Binary classification](resources/trump.jpg)\n",
    "Goal: Identify and categorize opinions from text data.\n",
    "---\n",
    "Following up you'll find a official dataset containing user tweets about the first 2016 GOP Presidential Debate, or as stated by the original source: \n",
    "```\n",
    "We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.\n",
    "```\n",
    "For more information about the data, please follow:\n",
    "https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment\n",
    "\n",
    "\n",
    "### Your goals are to complete the following tasks:\n",
    "---\n",
    "\n",
    "### 1. Parse the data and perform some exploration first, that is:\n",
    "- Load and initially explore the dataset located under: 'data/sentiment.csv'\n",
    "- remove all columns except 'text' and 'sentiment'\n",
    "- split data into 2 separate datasets, where one contains just negative and one contains the positive tweets(create 2 new datasets instead of deleting the old one)\n",
    "- remove stop words from tweets(see https://www.nltk.org/book/ch02.html 4.1), as well as hashtags, '@'-signs and 'RT'-sign'\n",
    "- print a wordcloud for the positive-tweet dataset aswell as the negative tweets dataset. It displays the most appearing words in each dataset. The code for the wordcloud is provided below.\n",
    "- create a list object, called 'tweets', that contains tuples: (list of words in tweet(cleaned), sentiment('Negative' or 'Positive'))\n",
    "\n",
    "### 2. Split the data into training and test set(the ratio should be 80% / 20%)\n",
    "    \n",
    "    \n",
    "### 3. Use the nltk library to train a NaiveBayesClassifier on the dataset:\n",
    "- use nltk.classify.util.apply_features to provide the word features to the classifier(look up docu https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util)\n",
    "the needed input argument feature_func input is provided by extract_features function.\n",
    "- Use: 'classifier = nltk.NaiveBayesClassifier.train(training_set)' to train the classifier, where training_set is the output of the apply_features function\n",
    "\n",
    "### 4. Use the trained classifier to predict the sentiments of the test set. Plot confusion matrix and print accuracy report like done above\n",
    "\n",
    "### 5. Provide your own inputs to the classifier and see check whether it is able to classify your intended sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               candidate  candidate_confidence relevant_yn  \\\n",
       "0   1  No candidate mentioned                   1.0         yes   \n",
       "1   2            Scott Walker                   1.0         yes   \n",
       "2   3  No candidate mentioned                   1.0         yes   \n",
       "3   4  No candidate mentioned                   1.0         yes   \n",
       "4   5            Donald Trump                   1.0         yes   \n",
       "\n",
       "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
       "0                     1.0   Neutral                0.6578  None of the above   \n",
       "1                     1.0  Positive                0.6333  None of the above   \n",
       "2                     1.0   Neutral                0.6629  None of the above   \n",
       "3                     1.0  Positive                1.0000  None of the above   \n",
       "4                     1.0  Positive                0.7045  None of the above   \n",
       "\n",
       "   subject_matter_confidence candidate_gold  ... relevant_yn_gold  \\\n",
       "0                     1.0000            NaN  ...              NaN   \n",
       "1                     1.0000            NaN  ...              NaN   \n",
       "2                     0.6629            NaN  ...              NaN   \n",
       "3                     0.7039            NaN  ...              NaN   \n",
       "4                     1.0000            NaN  ...              NaN   \n",
       "\n",
       "  retweet_count  sentiment_gold subject_matter_gold  \\\n",
       "0             5             NaN                 NaN   \n",
       "1            26             NaN                 NaN   \n",
       "2            27             NaN                 NaN   \n",
       "3           138             NaN                 NaN   \n",
       "4           156             NaN                 NaN   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "\n",
       "               tweet_created            tweet_id  tweet_location  \\\n",
       "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
       "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
       "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
       "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
       "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Central Time (US & Canada)  \n",
       "4                     Arizona  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/sentiment.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test set\n",
    "train, test = train_test_split(data,test_size = 0.2)\n",
    "# Removing neutral sentiments\n",
    "train = train[train.sentiment != \"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_pos = # TODO\n",
    "train_neg = # TODO\n",
    "\n",
    "test_pos = # TODO\n",
    "test_neg = # TODO\n",
    "\n",
    "def wordcloud(cleaned_words, color='black'):\n",
    "    words = ' '.join(cleaned_words)\n",
    "    cleaned_word = \" \".join([word for word in words.split()])\n",
    "    \n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Positive words\")\n",
    "wordcloud(train_pos,'white')\n",
    "print(\"Negative words\")\n",
    "wordcloud(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provided functions\n",
    "def get_words_in_tweets(tweets):\n",
    "    all = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "\n",
    "w_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "# IMPORTANT: USE THIS ONE FOR nltk.classify.apply_features argument input\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "\n",
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "        \n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (neg_cnt, len(test_neg)))        \n",
    "print('[Positive]: %s/%s '  % (pos_cnt, len(test_pos)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
