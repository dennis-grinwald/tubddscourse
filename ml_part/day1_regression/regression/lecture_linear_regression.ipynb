{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to supervised machine learning using regression methods\n",
    "---\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "$D=\\{(\\mathbf x_{1}, y_{1}), ..., (\\mathbf x_{N}, y_{N})\\}$,\n",
    "\n",
    "where $y_{i}\\epsilon \\mathbb{R}$\n",
    "\n",
    "## Goal of linear regression:\n",
    "Find a trend line that represents correlation of independent input varaibles and the output\n",
    "\n",
    "![Simple linear regression](resources/simple_regression.png)\n",
    "\n",
    "\n",
    "Here the x's are our input data points, and the y's are the labels that we wisch to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate linear regression\n",
    "---\n",
    "Intuition: Try to fit a line that \"best\" represents the trend of our data(e.g. correlation)\n",
    "\n",
    "$\\hat{y_{i}} = b+w_{1}x_{i}$\n",
    "\n",
    "$\\hat{y_{i}}$: predicted value for data point $x_{i}$\n",
    "\n",
    "$x_{i}$: independent variable/feature of training data point $i$\n",
    "\n",
    "$b$: bias value (trainable)\n",
    "\n",
    "$w_{1}$: weight value for feature x (trainable)\n",
    "\n",
    "### Note: Bias and weight value stay the same for every data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "X = 2.5 * np.random.rand(100) + 1.5   \n",
    "Y = 0.3 * X \n",
    "\n",
    "model = {'weight': 0.3}\n",
    "\n",
    "res = 1 * np.random.randn(100)      \n",
    "Y_observed = 0.3 * X + res \n",
    "\n",
    "# Create pandas dataframe to store our X and y values\n",
    "df = pd.DataFrame(\n",
    "    {'experience': X,\n",
    "     'income[thous.]': Y_observed}\n",
    ")\n",
    "\n",
    "# Show the first five rows of our dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "---\n",
    "### \"True\" underlying data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.xlabel('Experience[yrs]')\n",
    "plt.ylabel('Income/month[thous.]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed, noisy real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(X, Y_observed)\n",
    "plt.xlabel('Experience[yrs]')\n",
    "plt.ylabel('Income/month[thous.]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does interpolation finds a good trend representation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "f = interp1d(X[np.argsort(X)], Y_observed[np.argsort(X)], kind='cubic')\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(X, Y_observed,'o', X, Y, X[np.argsort(X)], f(X[np.argsort(X)]), '--')\n",
    "\n",
    "plt.xlabel('Experience[yrs]')\n",
    "plt.ylabel('Income/month[thous.]')\n",
    "plt.legend(['observed data', 'real data', 'interpolation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As indicated in the plot above, linear interpolation does quite bad fitting that function, because how would we predict a new point ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to measure the error wrt. our predictions and the real labels ?\n",
    "\n",
    "## -> Residuals\n",
    "A residual (or fitting deviation), is an observable estimate of the unobservable statistical error. - https://en.wikipedia.org/wiki/Errors_and_residuals\n",
    "\n",
    "$res = \\hat{y}-y$\n",
    "\n",
    "where: \n",
    "\n",
    "$\\hat{y}=w_{1}x$: predicted values\n",
    "\n",
    "$y$: observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(X, model, Y_observed):\n",
    "    return model['weight'] * X - Y_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.scatter(X, Y_observed)\n",
    "plt.plot(X, Y, 'k')\n",
    "plt.vlines(X, Y_observed, Y_observed+residuals(X, model, Y_observed), 'r')\n",
    "plt.xlabel('experience')\n",
    "plt.ylabel('income')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Loss funtion / error measure:\n",
    "## Mean-squared error\n",
    "\n",
    "$MSE_{scalar} = \\frac{1}{2N} \\sum_{n=1}^{N} ( wx - y )^{2} $\n",
    "\n",
    "$MSE_{matrix} = \\frac{1}{2N} \\lvert \\lvert \\mathbf{X}w_{1} - \\mathbf{y} \\lvert \\lvert^{2} $\n",
    "\n",
    "with:\n",
    "\n",
    "$\\mathbf{X}\\epsilon \\mathbb{R}^{Nx1}$, N: number of data points\n",
    "\n",
    "$\\mathbf{y}\\epsilon \\mathbb{R}^{Nx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformulating the task as an optimization problem\n",
    "\n",
    "Let:\n",
    "\n",
    "$MSE = \\underset{w_{1}}{min} \\ \\frac{1}{2N} \\sum_{n=1}^{N} ( \\hat{y} - y )^{2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_s = np.linspace(-10, 10, 20)\n",
    "\n",
    "mse_losses = []\n",
    "for w in w_s:\n",
    "    model = {\n",
    "        'weight': w\n",
    "    }\n",
    "    \n",
    "    Y_pred = w*X\n",
    "    mse_losses.append(mean_squared_error(Y_observed, Y_pred))\n",
    "    \n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.plot(w_s, mse_losses)\n",
    "plt.xlabel('Weight vector value')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex optimization problem -> there exists a global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical optimization problem formulation:\n",
    "\n",
    "$\\underset{w_{1}}{min}\\ MSE = \\underset{w_{1}}{min} \\ \\frac{1}{2n} \\sum_{n=1}^{N} ( w_{1}x_{i} - y)^{2} = \\underset{w_{1}}{min} \\ \\frac{1}{2N} \\lvert \\lvert \\mathbf{X}w_{1} - \\mathbf{y} \\lvert \\lvert^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Approach: Closed-form solution (Rarely possible !)\n",
    "\n",
    "In order to minimize set: $\\nabla_{w_{1}} MSE = 0$ and solve for $w_{1}$\n",
    "\n",
    "$\\nabla_{w_{1}} MSE = \\frac{(w_{1}\\mathbf{X^{T}X} - \\mathbf{X^{T}y}) \\overset{}{}}{N}= 0$\n",
    "\n",
    "$\\Rightarrow w_{1}= \\frac{\\mathbf{X}^{T}\\mathbf{y}(\\mathbf{X}^{T}\\mathbf{X})^{-1}}{N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.dot(X.T, Y_observed)/(np.dot(X.T,X))\n",
    "print(f'Optimial weight value: {w_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.plot(X, Y,'r', X, w_opt*X, X, Y_observed, 'o')\n",
    "plt.legend(['true model', 'estimated model', 'observed data'])\n",
    "plt.xlabel('experience')\n",
    "plt.ylabel('income')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approach: Gradient Descent - Very important optimization algorithm\n",
    "\n",
    "<img src=\"resources/negative_gradient.svg\"  />\n",
    "<img src=\"resources/negative_gradient2.svg\"  />\n",
    "\n",
    "## How it works\n",
    "\n",
    "1. Randomly initialize $w_{1}$\n",
    "2. Repeat until convergence: $w^{t+1}_{1} \\leftarrow w^{t}_{1} - \\alpha \\nabla_{w^{t}_{1}}J(w_{1}^{t})$, \n",
    "where $\\alpha$= learning rate and $J(w_{1})=MSE$\n",
    "\n",
    "## NOTE:\n",
    "- $\\nabla_{w} J(w)$ is the vector of greatest increase of $J(w)$, when starting from point $w$.\n",
    "- $-\\nabla_{w} J(w)$ is the vector of greatest decrease of $J(w)$, when starting from point $w$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Usually loss functions are NON-convex, meaning that there doesn't exist a closed form solution\n",
    "\n",
    "<img src=\"resources/grad_descent.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of learning rate on the optimization performance\n",
    "The learning rate $\\alpha$, also often called \"step size\", determines how big of your update steps will be in the direction of the current gradient. \n",
    "\n",
    "Important facts about the learning rate:\n",
    "---\n",
    "1. Smaller learning rates will converge slower to a optimum\n",
    "2. Larger learning rates might overshoot the optimum and thus never converge towards an optimum\n",
    "3. The learning rate is a hyperparameter(not learnable, thus needs to be decided by the user)\n",
    "\n",
    "\n",
    "### -> Jump to Google Viz.:\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to linear regression... how do we evaluate the goodness of our trained model ?\n",
    "\n",
    "\n",
    "### Workflow\n",
    "- Load Salary data\n",
    "- do train/test split\n",
    "- fit linear regressor on training data\n",
    "- predict test data\n",
    "\n",
    "### Linear regression evaluation\n",
    "1. Approach: Visual comparison\n",
    "2. Approach: MSE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Salary_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[:int(data.shape[0]*0.8)]\n",
    "\n",
    "test_data = data.iloc[int(data.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.iloc[:,0].as_matrix().reshape(-1,1)\n",
    "y_train = train_data.iloc[:,1].as_matrix().reshape(-1,1)\n",
    "\n",
    "x_test = test_data.iloc[:,0].as_matrix().reshape(-1,1)\n",
    "y_test = test_data.iloc[:,1].as_matrix().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = trained_model.predict(x_train)\n",
    "\n",
    "test_predictions = trained_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Approach: Visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.iloc[:,0], data.iloc[:,1])\n",
    "\n",
    "plt.scatter(x_train, train_predictions)\n",
    "\n",
    "plt.scatter(x_test, test_predictions)\n",
    "\n",
    "plt.legend(['true data', 'train predictions', 'test predictions'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Approach: Train & Test residual and MSE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_residuals = y_train - train_predictions\n",
    "\n",
    "summed_train_residuals = np.sum(train_residuals)\n",
    "\n",
    "print(summed_train_residuals)\n",
    "\n",
    "test_residuals = y_test - test_predictions\n",
    "summed_test_residuals = np.sum(test_residuals)\n",
    "print(summed_test_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_train, train_predictions))\n",
    "\n",
    "print(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residuals are almost zero -> cool\n",
    "They should actually reach zero if our test set size grows\n",
    "### MSE is almost the same -> no overfitting -> cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaways:\n",
    "---\n",
    "\n",
    "## 1. Define a model that represents the trend\n",
    "## 2. Define an error measure/loss function\n",
    "## 3. Use mathematical optimization(e.g. Gradient Descent) to minimize the loss\n",
    "## 4. Split data into train and test set to be able to evaluate your model's generalization performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
